{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11678705",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([68, 48, 3])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch import Tensor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class LandmarkDataset(Dataset):\n",
    "    def __init__(self, landmark_file, label_file, label_vec_file):\n",
    "        super().__init__()\n",
    "        self.landmarks = np.load(landmark_file)\n",
    "        self.labels = pd.read_json(label_file)[[\"gloss\"]]\n",
    "        self.label_vec = np.load(label_vec_file)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        landmarks = Tensor(self.landmarks.get(str(index)))\n",
    "        word = self.labels.loc[index].iloc[0]\n",
    "        word_vec = Tensor(self.label_vec.get(word))\n",
    "        return landmarks, word, word_vec\n",
    "    \n",
    "ds = LandmarkDataset(\"landmarks_V2.npz\", \"WLASL_parsed_data.json\", \"word_vec.npz\")\n",
    "val, word, word_vec = ds[2]\n",
    "val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77f17f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for encoding the labels\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import fasttext\n",
    "\n",
    "# labels = pd.read_json(\"WLASL_parsed_data.json\").loc[:, \"gloss\"]\n",
    "# fasttext.util.download_model('en', if_exists='ignore')\n",
    "# ft = fasttext.load_model('cc.en.300.bin')\n",
    "# encoded_labels = {label: ft.get_word_vector(label) for label in labels}\n",
    "# np.savez_compressed(\"word_vec.npz\", **encoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537985c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([119, 64, 48, 3])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "\n",
    "TEST_SIZE = 0.2\n",
    "BATCH_SIZE = 32\n",
    "SEED = 10\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "train_i, test_i = train_test_split(\n",
    "    range(len(ds)),\n",
    "    stratify=ds.labels,\n",
    "    test_size=TEST_SIZE,\n",
    "    random_state=SEED\n",
    "    )\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "train_split = Subset(ds, train_i)\n",
    "test_split = Subset(ds, test_i)\n",
    "\n",
    "def collate(batch):\n",
    "    batch.sort(key=lambda x: x[0].shape[0], reverse=True)\n",
    "    \n",
    "    landmarks, words, word_vecs = zip(*batch)\n",
    "    lengths = [len(lm) for lm in landmarks]\n",
    "\n",
    "    padded_landmarks = rnn_utils.pad_sequence(landmarks).to(DEVICE)\n",
    "    word_vecs = torch.stack(word_vecs).to(DEVICE)\n",
    "    \n",
    "    return \\\n",
    "        padded_landmarks, \\\n",
    "        lengths, \\\n",
    "        list(words), \\\n",
    "        word_vecs\n",
    "\n",
    "train_batches = DataLoader(train_split, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate)\n",
    "test_batches = DataLoader(test_split, batch_size=BATCH_SIZE, collate_fn=collate)\n",
    "\n",
    "landmarks, lengths, labels, label_vecs = next(iter(train_batches))\n",
    "landmarks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72886eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\skipz\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\lazy.py:181: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0061, -0.0030,  0.0518,  ...,  0.0022,  0.0323, -0.0005],\n",
       "        [-0.0061, -0.0004,  0.0514,  ...,  0.0004,  0.0322,  0.0055],\n",
       "        [-0.0065, -0.0021,  0.0505,  ...,  0.0030,  0.0336,  0.0027],\n",
       "        ...,\n",
       "        [-0.0080,  0.0218,  0.0731,  ...,  0.0183,  0.0470,  0.0044],\n",
       "        [-0.0079, -0.0003,  0.0414,  ..., -0.0036,  0.0359, -0.0054],\n",
       "        [-0.0217,  0.0347,  0.0975,  ...,  0.0294,  0.0603, -0.0128]],\n",
       "       device='cuda:0', grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class TestModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.LazyLinear(200),\n",
    "            nn.LeakyReLU(),            \n",
    "            nn.LazyLinear(300),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        self.rnn = nn.GRU(300, 300)\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.LazyLinear(300),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.LazyLinear(300),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.LazyLinear(300),\n",
    "        )\n",
    "\n",
    "    def forward(self, landmarks, lengths):\n",
    "        frames, batch, lm, pos = landmarks.shape\n",
    "        x = landmarks.view(frames, batch, lm * pos)\n",
    "        x = self.fc1(x)\n",
    "        x = rnn_utils.pack_padded_sequence(x, lengths)\n",
    "        x, h = self.rnn(x)\n",
    "        x = self.fc2(h)\n",
    "        x = h.squeeze(0)\n",
    "        return x\n",
    "\n",
    "model = TestModel().to(DEVICE)\n",
    "\n",
    "landmarks, lengths, labels, label_vecs = next(iter(train_batches))\n",
    "\n",
    "outp = model(landmarks, lengths)\n",
    "outp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3f6f515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class VecToWord:\n",
    "    def __init__(self, ds, device):\n",
    "        self.vectors = torch.tensor(np.array(list(ds.label_vec.values())), device=DEVICE)\n",
    "        self.vectors.requires_grad_(False)\n",
    "\n",
    "        self.dictionary = list(ds.label_vec.keys())\n",
    "    \n",
    "    def to_word(self, vec):\n",
    "        classes = self.to_class(vec).tolist()\n",
    "        return [self.dictionary[c] for c in classes]\n",
    "        \n",
    "    def get_acc(self, vec, target_words):\n",
    "        return sum(pred == targ for pred, targ in zip(self.to_word(vec), target_words))\n",
    "\n",
    "\n",
    "    def to_class(self, vec):\n",
    "        res = F.cosine_similarity(\n",
    "            vec.unsqueeze(1).repeat(1, 2000, 1), \n",
    "            self.vectors.repeat(vec.shape[0], 1, 1), \n",
    "            dim=2)\n",
    "        return res.argmax(1)\n",
    "\n",
    "v2w = VecToWord(ds, DEVICE)\n",
    "v2w.get_acc(outp, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42d7dbd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\skipz\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "100%|██████████| 150/150 [00:15<00:00,  9.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.005274246347447237, Accuracy: 5.333333%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [00:15<00:00,  9.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Loss: 0.0046615773734326165, Accuracy: 2.666667%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [00:15<00:00,  9.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Loss: 0.004621435410032669, Accuracy: 4.666667%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [00:15<00:00,  9.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Loss: 0.004611053070984781, Accuracy: 4.666667%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [00:14<00:00, 10.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Loss: 0.004603677638806402, Accuracy: 4.666667%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "from torch.nn import MSELoss\n",
    "from tqdm import tqdm\n",
    "\n",
    "criterion = MSELoss()\n",
    "optimizer = Adam(model.parameters(), lr=0.001, eps=1e-3) # lower eps cause of f16\n",
    "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "losses = []\n",
    "accuracies = []\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    for inputs, lengths, words, word_vec in tqdm(train_batches, smoothing=0.8):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, lengths)\n",
    "\n",
    "        loss = criterion(outputs, word_vec)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        correct += v2w.get_acc(outputs, words)\n",
    "\n",
    "    loss = running_loss/ len(train_batches)\n",
    "    acc = 100 * correct / len(train_batches)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss}, Accuracy: {acc:03f}%\")\n",
    "    losses.append(loss)\n",
    "    accuracies.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1509cb2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38/38 [00:02<00:00, 16.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.000000%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, lengths, words, word_vec in tqdm(test_batches, smoothing=0.8):\n",
    "        outputs = model(inputs, lengths)\n",
    "\n",
    "        correct += v2w.get_acc(outputs, words)\n",
    "\n",
    "print(f\"Accuracy: {100 * correct / len(test_batches):03f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
