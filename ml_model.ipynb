{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\skipz\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\skipz\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.10.0.84)\n",
      "Requirement already satisfied: numpy in c:\\users\\skipz\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: pillow in c:\\users\\skipz\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (10.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\skipz\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\skipz\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\skipz\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\skipz\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install pandas opencv-python numpy pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\skipz\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\io\\video.py:161: UserWarning: The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\n",
      "  warnings.warn(\"The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\")\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'av' has no attribute 'AVError'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\skipz\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\io\\video.py:200\u001b[0m, in \u001b[0;36m_read_from_stream\u001b[1;34m(container, start_offset, end_offset, pts_unit, stream, stream_name)\u001b[0m\n\u001b[0;32m    199\u001b[0m frames[frame\u001b[38;5;241m.\u001b[39mpts] \u001b[38;5;241m=\u001b[39m frame\n\u001b[1;32m--> 200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m frame\u001b[38;5;241m.\u001b[39mpts \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m end_offset:\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m should_buffer \u001b[38;5;129;01mand\u001b[39;00m buffer_count \u001b[38;5;241m<\u001b[39m max_buffer_size:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\skipz\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\io\\video.py:296\u001b[0m, in \u001b[0;36mread_video\u001b[1;34m(filename, start_pts, end_pts, pts_unit, output_format)\u001b[0m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m container\u001b[38;5;241m.\u001b[39mstreams\u001b[38;5;241m.\u001b[39mvideo:\n\u001b[1;32m--> 296\u001b[0m     video_frames \u001b[38;5;241m=\u001b[39m \u001b[43m_read_from_stream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstart_pts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    299\u001b[0m \u001b[43m        \u001b[49m\u001b[43mend_pts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    300\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpts_unit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    301\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstreams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvideo\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    302\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvideo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    303\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    304\u001b[0m     video_fps \u001b[38;5;241m=\u001b[39m container\u001b[38;5;241m.\u001b[39mstreams\u001b[38;5;241m.\u001b[39mvideo[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39maverage_rate\n",
      "File \u001b[1;32mc:\\Users\\skipz\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\io\\video.py:205\u001b[0m, in \u001b[0;36m_read_from_stream\u001b[1;34m(container, start_offset, end_offset, pts_unit, stream, stream_name)\u001b[0m\n\u001b[0;32m    204\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 205\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[43mav\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAVError\u001b[49m:\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;66;03m# TODO add a warning\u001b[39;00m\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'av' has no attribute 'AVError'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[137], line 57\u001b[0m\n\u001b[0;32m     54\u001b[0m     get_first_last(ds[i][\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28mprint\u001b[39m(ds\u001b[38;5;241m.\u001b[39mannotations\u001b[38;5;241m.\u001b[39mloc[i])\n\u001b[1;32m---> 57\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mVideoDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabels.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwlasl_downsampled.mp4\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m stats(\u001b[38;5;241m0\u001b[39m)\n",
      "Cell \u001b[1;32mIn[137], line 19\u001b[0m, in \u001b[0;36mVideoDataset.__init__\u001b[1;34m(self, annotations_file, vid_file, device)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_map \u001b[38;5;241m=\u001b[39m {c: i \u001b[38;5;28;01mfor\u001b[39;00m i, c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mset\u001b[39m(labels))}\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_map[s] \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m labels])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvid \u001b[38;5;241m=\u001b[39m \u001b[43mread_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvid_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvid\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcap \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mVideoCapture(vid_file)\n",
      "File \u001b[1;32mc:\\Users\\skipz\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\io\\video.py:320\u001b[0m, in \u001b[0;36mread_video\u001b[1;34m(filename, start_pts, end_pts, pts_unit, output_format)\u001b[0m\n\u001b[0;32m    310\u001b[0m             audio_frames \u001b[38;5;241m=\u001b[39m _read_from_stream(\n\u001b[0;32m    311\u001b[0m                 container,\n\u001b[0;32m    312\u001b[0m                 start_pts,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    316\u001b[0m                 {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m},\n\u001b[0;32m    317\u001b[0m             )\n\u001b[0;32m    318\u001b[0m             info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio_fps\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m container\u001b[38;5;241m.\u001b[39mstreams\u001b[38;5;241m.\u001b[39maudio[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mrate\n\u001b[1;32m--> 320\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[43mav\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAVError\u001b[49m:\n\u001b[0;32m    321\u001b[0m     \u001b[38;5;66;03m# TODO raise a warning?\u001b[39;00m\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    324\u001b[0m vframes_list \u001b[38;5;241m=\u001b[39m [frame\u001b[38;5;241m.\u001b[39mto_rgb()\u001b[38;5;241m.\u001b[39mto_ndarray() \u001b[38;5;28;01mfor\u001b[39;00m frame \u001b[38;5;129;01min\u001b[39;00m video_frames]\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'av' has no attribute 'AVError'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, Subset, DataLoader\n",
    "from IPython import display\n",
    "import torch\n",
    "import time\n",
    "from PIL import Image\n",
    "\n",
    "DEVICE = 'cuda'\n",
    "\n",
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, annotations_file, vid_file, device):\n",
    "        self.annotations = pd.read_csv(annotations_file)\n",
    "        labels = self.annotations['label']\n",
    "        self.label_map = {c: i for i, c in enumerate(set(labels))}\n",
    "        self.labels = torch.tensor([self.label_map[s] for s in labels]).to(device)\n",
    "        self.cap = cv2.VideoCapture(vid_file)\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "    \n",
    "    def get_clip(self, start, length):\n",
    "        self.cap.set(cv2.CAP_PROP_POS_FRAMES, start)\n",
    "        \n",
    "        return np.expand_dims(np.array([self.cap.read()[1][:,:,0] for _ in range(length)]).astype(np.float32), 1)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.annotations.loc[idx]\n",
    "        return torch.tensor(self.get_clip(row['offset'], row['length'])).to(self.device), self.labels[idx]\n",
    "\n",
    "def show_frame(frame):\n",
    "    frame.cpu()\n",
    "    display.display(Image.fromarray(frame.numpy().astype(np.uint8)))\n",
    "\n",
    "def play_video(vid: torch.Tensor):\n",
    "    for frame in vid[:,0,:,:]:\n",
    "        display.clear_output(wait=True)\n",
    "        show_frame(frame)\n",
    "        time.sleep(1/15)\n",
    "    \n",
    "def get_first_last(vid):\n",
    "    frame_start = vid[0][0].cpu()\n",
    "    frame_end = vid[-1][0].cpu()\n",
    "    \n",
    "    show_frame(frame_start)\n",
    "    show_frame(frame_end)\n",
    "\n",
    "def stats(i):\n",
    "    get_first_last(ds[i][0])\n",
    "    print(ds.annotations.loc[i])\n",
    "        \n",
    "ds = VideoDataset('labels.csv', 'wlasl_downsampled.mp4', DEVICE)\n",
    "\n",
    "stats(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "\n",
    "TEST_SIZE = 0.8\n",
    "BATCH_SIZE = 64\n",
    "SEED = 10\n",
    "\n",
    "train_i, test_i = train_test_split(\n",
    "    range(len(ds)),\n",
    "    stratify=ds.annotations['label'],\n",
    "    test_size=TEST_SIZE,\n",
    "    random_state=SEED\n",
    "    )\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "train_split = Subset(ds, train_i)\n",
    "test_split = Subset(ds, test_i)\n",
    "\n",
    "def collate(batch):\n",
    "    batch.sort(key=lambda x: x[0].size(0), reverse=True)\n",
    "    videos, labels = zip(*batch)\n",
    "\n",
    "    lengths = [video.size(0) for video in videos]\n",
    "\n",
    "    padded_videos = rnn_utils.pad_sequence(videos)\n",
    "    \n",
    "    return padded_videos, lengths, torch.stack(labels)\n",
    "\n",
    "train_batches = DataLoader(train_split, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate)\n",
    "test_batches = DataLoader(test_split, batch_size=BATCH_SIZE, collate_fn=collate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flatten: torch.Size([110, 32, 1, 136, 242]), torch.Size([3520, 1, 136, 242])\n",
      "MaxPool2d: torch.Size([3520, 1, 136, 242]), torch.Size([3520, 1, 34, 60])\n",
      "BatchNorm2d: torch.Size([3520, 1, 34, 60]), torch.Size([3520, 1, 34, 60])\n",
      "Conv2d: torch.Size([3520, 1, 34, 60]), torch.Size([3520, 1, 30, 56])\n",
      "Flatten: torch.Size([3520, 1, 30, 56]), torch.Size([3520, 1680])\n",
      "Linear: torch.Size([3520, 1680]), torch.Size([3520, 100])\n",
      "ReLU: torch.Size([3520, 100]), torch.Size([3520, 100])\n",
      "BatchNorm1d: torch.Size([3520, 100]), torch.Size([3520, 100])\n",
      "BatchNorm1d: torch.Size([32, 100]), torch.Size([32, 100])\n",
      "Linear: torch.Size([32, 100]), torch.Size([32, 100])\n",
      "ReLU: torch.Size([32, 100]), torch.Size([32, 100])\n",
      "BatchNorm1d: torch.Size([32, 100]), torch.Size([32, 100])\n",
      "Linear: torch.Size([32, 100]), torch.Size([32, 100])\n",
      "ReLU: torch.Size([32, 100]), torch.Size([32, 100])\n",
      "BatchNorm1d: torch.Size([32, 100]), torch.Size([32, 100])\n",
      "Linear: torch.Size([32, 100]), torch.Size([32, 2000])\n",
      "Softmax: torch.Size([32, 2000]), torch.Size([32, 2000])\n",
      "torch.Size([32, 2000])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class TransformerModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers_2d = nn.Sequential(\n",
    "            nn.Flatten(0, 1),\n",
    "            nn.MaxPool2d(4),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.Conv2d(1, 1, 5),\n",
    "            )\n",
    "        \n",
    "        self.layers_1d = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1680, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(100),\n",
    "        )\n",
    "\n",
    "        self.rnn = nn.GRU(100, 100)\n",
    "\n",
    "        self.output = nn.Sequential(\n",
    "            nn.BatchNorm1d(100),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(100),\n",
    "            nn.Linear(100, 2000),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, inp, lengths):\n",
    "        x = inp\n",
    "        frames, batch, channels, h, w = x.shape\n",
    "        x = self.layers_2d(x)\n",
    "        x = self.layers_1d(x)\n",
    "        x = x.view(frames, batch, -1)\n",
    "        x = rnn_utils.pack_padded_sequence(x, lengths, enforce_sorted=False)\n",
    "        x, h = self.rnn(x)\n",
    "        x = h[0, :, :]\n",
    "        output = self.output(x)\n",
    "        \n",
    "        return output\n",
    "\n",
    "model = TransformerModule()\n",
    "\n",
    "def print_hook(module, args, output):\n",
    "    inp = args[0]\n",
    "    if isinstance(inp, torch.Tensor):\n",
    "        inp = inp.shape\n",
    "    elif isinstance(inp, rnn_utils.PackedSequence) :\n",
    "        inp = inp.data.shape\n",
    "    \n",
    "    if isinstance(output, tuple):\n",
    "        output = output[0]\n",
    "    outp = output.shape\n",
    "    name = type(module).__name__\n",
    "    print(f'{name}: {inp}, {outp}')\n",
    "\n",
    "handles = []\n",
    "\n",
    "for child in model.children():\n",
    "    if child.children():\n",
    "        for child_ in child.children():\n",
    "            handles.append(child_.register_forward_hook(print_hook))\n",
    "    else:\n",
    "        handles.append(child.register_forward_hook(print_hook))\n",
    "\n",
    "sample = rnn_utils.pad_sequence(torch.rand([32, 110, 1, 136, 242]))\n",
    "lengths = torch.randint(1, 100, (32,))\n",
    "\n",
    "result = model(sample, lengths)\n",
    "\n",
    "print(result.shape)\n",
    "\n",
    "for handle in handles:\n",
    "    handle.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [00:19<00:00,  1.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, Loss: 7.600675711760649\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                          model_forward         4.60%     952.701ms        96.20%       19.934s       19.934s        1.515s         7.10%       19.932s       19.932s             1  \n",
      "enumerate(DataLoader)#_SingleProcessDataLoaderIter._...        71.21%       14.754s        84.26%       17.458s     459.419ms       14.645s        68.66%       17.458s     459.421ms            38  \n",
      "                                               aten::to         0.56%     117.036ms        11.64%        2.412s     492.469us     110.161ms         0.52%        2.128s     434.625us          4897  \n",
      "                                         aten::_to_copy         1.24%     256.885ms        11.07%        2.295s     485.627us     182.536ms         0.86%        2.018s     427.132us          4725  \n",
      "                                            aten::copy_        10.08%        2.088s        10.08%        2.088s     204.714us        1.880s         8.82%        1.880s     184.352us         10199  \n",
      "                                             aten::item         0.12%      25.840ms         3.12%     647.095ms     342.560us      41.094ms         0.19%      61.959ms      32.800us          1889  \n",
      "                              aten::_local_scalar_dense         3.00%     621.255ms         3.00%     621.255ms     328.880us      20.865ms         0.10%      20.865ms      11.046us          1889  \n",
      "autograd::engine::evaluate_function: PackPaddedSeque...         0.01%       1.196ms         1.98%     411.206ms      11.114ms     897.000us         0.00%     412.927ms      11.160ms            37  \n",
      "                            PackPaddedSequenceBackward0         0.00%     751.000us         1.98%     410.010ms      11.081ms     895.000us         0.00%     412.030ms      11.136ms            37  \n",
      "                   aten::_pack_padded_sequence_backward         0.96%     198.528ms         1.98%     409.259ms      11.061ms     112.530ms         0.53%     411.135ms      11.112ms            37  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 20.720s\n",
      "Self CUDA time total: 21.328s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from tqdm import tqdm\n",
    "from torch import profiler\n",
    "\n",
    "criterion = CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "model.to(DEVICE)\n",
    "\n",
    "\n",
    "num_epochs = 1\n",
    "\n",
    "with profiler.profile(\n",
    "    activities=[\n",
    "        profiler.ProfilerActivity.CPU,\n",
    "        profiler.ProfilerActivity.CUDA,\n",
    "    ]\n",
    ") as prof:\n",
    "    with profiler.record_function(\"model_forward\"):\n",
    "        model.train()\n",
    "        for epoch in range(num_epochs):\n",
    "            running_loss = 0.0\n",
    "            for inputs, lengths, annotations in tqdm(train_batches, smoothing=0.8):\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs, lengths)\n",
    "                loss = criterion(outputs, annotations)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_batches)}\")\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))\n",
    "\n",
    "\n",
    "# model.eval()\n",
    "# correct = 0\n",
    "# total = 0\n",
    "# with torch.no_grad():\n",
    "#     for inputs, lengths, annotations in tqdm(test_batches, smoothing=0.8):\n",
    "#         outputs = model(inputs, lengths)\n",
    "#         _, predicted = torch.max(outputs.data, 1)\n",
    "#         total += annotations.size(0)\n",
    "#         correct += (predicted == annotations).sum().item()\n",
    "\n",
    "# print(f\"Accuracy: {100 * correct / total}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
